{
  "name": "ERP Reliability Analysis Toolbox",
  "tagline": "Matlab toolbox for obtaining dependability estimates for ERP measurements",
  "body": "\r\nThe ERP Reliability Analysis (ERA) toolbox is an open-source Matlab program that uses generalizability (G) theory to evaluate the reliability of ERP data. The purpose of the toolbox is to characterize the dependability (G-theory analog of reliability) of ERP scores to facilitate their calculation on a study-by-study basis and increase the reporting of these estimates.\r\n\r\nThe ERA Toolbox provides information about the minimum number of trials needed for dependable ERP scores and describes the overall dependability of ERP measurements. All information provided by the ERA Toolbox is stratified by group and condition to allow the user to directly compare dependability (e.g., a particular group may require more trials to achieve an acceptable level of dependability than another group). The code used by the toolbox was based on the formulas discussed in detail by [Baldwin, Larson, and Clayson (2015)](http://onlinelibrary.wiley.com/doi/10.1111/psyp.12401/abstract).\r\n\r\nCheck out the [wiki](https://github.com/peclayson/ERA_Toolbox/wiki).\r\n\r\n### Why another toolbox?\r\nReliability is a property of scores (the data in hand), not a property of measures. This means that P3, error-related negativity (ERN), late positive potential (LPP), (insert your favorite ERP component here) is not reliable in some \"universal\" sense. Since reliability is context dependent, demonstrating the reliability of LPP scores in undergraduates at UCLA does not mean LPP scores recorded from children in New York can be assumed to be reliable. Measurement reliability needs to be demonstrated on a population-by-population, study-by-study, component-by-component basis.\r\n\r\nThe purpose of the ERA Toolbox is to facilitate the calculation of dependability estimates to characterize observed ERP scores. ERP psychometric studies have been useful in suggesting cutoffs and characterizing the overall reliability of ERP components in those studies. When designing a study, that information can help guide decisions about, for example, the number of trials to present to a participant for a given population. However, just because the observed data meet the previously recommended trial cutoff does not mean that the ERP scores are necessarily reliable. ERP score reliability cannot be inferred from trial counts, despite that trial counts and reliability are certainly related.\r\n\r\nMy hope is that the ERA Toolbox will make it easier to demonstrate the reliability of ERP scores on a study-by-study basis.\r\n\r\nMismeasurement of ERPs leads to misunderstood phenomena and mistaken conclusions. Poor ERP score reliability from mismeasurement compromises validity. Improving ERP measurement, by ensuring score reliability, can improve our trust of the inferences drawn from observed scores and the likelihood of our findings replicating.\r\n\r\n### A Little History\r\nThis project was started in December 2015 by Peter Clayson (@peclayson). What started as some in-house code turned into a flexible gui that I hope can help others. After all, there's no need to rewrite the wheel! ",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}